{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['OPENAI_API_KEY'] = '****'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '****'\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'task'\n",
    "\n",
    "#PDF 로드 함수\n",
    "def load_pdf(file_path):\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        return [doc.page_content for doc in docs]\n",
    "    except Exception as e:\n",
    "        print(f\"PDF 로드 중 오류 발생: {e}\")\n",
    "        return None\n",
    "\n",
    "# 텍스트 Splitter 함수\n",
    "def split_text(documents, splitter_type=\"recursive\", chunk_size=500, chunk_overlap=0):\n",
    "    try:\n",
    "        # splitter_type에 따라 텍스트 분리기 선택\n",
    "        if splitter_type == \"character\":\n",
    "            text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        elif splitter_type == \"recursive\":\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        else:\n",
    "            raise ValueError(\"지원되지 않는 Splitter 타입입니다.\")\n",
    "        \n",
    "        texts = text_splitter.create_documents(documents)\n",
    "        return [text.page_content for text in texts]\n",
    "    except Exception as e:\n",
    "        print(f\"텍스트 분리 중 오류 발생: {e}\")\n",
    "        return None\n",
    "\n",
    "# Embedding \n",
    "def initialize_embedding(model_name=\"intfloat/multilingual-e5-large-instruct\"):\n",
    "    try:\n",
    "        hf_embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={\"device\": \"cpu\"},\n",
    "            encode_kwargs={\"normalize_embeddings\": True},\n",
    "        )\n",
    "        return hf_embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding 초기화 중 오류 발생: {e}\")\n",
    "        return None\n",
    "\n",
    "# Search_query\n",
    "def search_query(query, texts, embeddings):\n",
    "    try:\n",
    "        embedded_query = embeddings.embed_query(query)\n",
    "        embedded_documents = embeddings.embed_documents(texts)\n",
    "\n",
    "        if len(embedded_query) == 0 or len(embedded_documents) == 0:\n",
    "            raise ValueError(\"Embedding 결과가 비어 있습니다.\")\n",
    "\n",
    "        # 유사도 계산 (코사인 유사도)\n",
    "        scores = np.dot(np.array(embedded_query), np.array(embedded_documents).T)\n",
    "        sorted_idx = scores.argsort()[::-1]  # 높은 점수 순으로 정렬\n",
    "\n",
    "        print(f\"[Query] {query}\\n{'='*40}\")\n",
    "        for i, idx in enumerate(sorted_idx[:5]):  # 상위 5개 출력\n",
    "            print(f\"[{i}] (유사도: {scores[idx]:.4f}) {texts[idx]}\")\n",
    "        return sorted_idx\n",
    "    except Exception as e:\n",
    "        print(f\"검색 중 오류 발생: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query] \n",
      "                    what is this?\n",
      "                    \n",
      "========================================\n",
      "[0] (유사도: 0.7738) Rewritten SetRewriting\n",
      "Supervised \n",
      "Fine-TuningRaw Text\n",
      "Text: # Confidence Interval calculation for \n",
      "Power Density Estimation in MATLAB\n",
      "First of all, I am new to these statistics stuff but \n",
      "very interested in the background. I try to……\n",
      "Original SFT Dataset\n",
      "Figure 2: The pipeline of AITP. AITP first generates a difference set, then rewrites the raw text into instruction-response\n",
      "pairs to form a rewritten set, and finally combines the rewritten set with the original SFT dataset for model training.\n",
      "[1] (유사도: 0.7693) comparing their distribution to that of the pre-training cor-\n",
      "pus. Underrepresented data is then rewritten into high-\n",
      "quality instruction-response pairs, enhancing dataset cover-\n",
      "age and alignment. As shown in Figure 2, AITP involves\n",
      "three stages: (1) generating a difference set based on density\n",
      "comparisons, (2) rewriting raw text into instruction-response\n",
      "1arXiv:2501.09368v2  [cs.AI]  17 Jan 2025\n",
      "[2] (유사도: 0.7691) points, hx,hyandσare bandwidth parameters that con-\n",
      "trol the smoothness in the x direction, y direction and kernel\n",
      "respectively. The KDE visualization highlights distribution\n",
      "differences, identifying regions of divergence between the\n",
      "pretraining and SFT datasets.\n",
      "2\n",
      "[3] (유사도: 0.7661) Aligning Instruction Tuning with Pre-training\n",
      "Yiming Liang* 1 2 3Tianyu Zheng* 4 5Xinrun Du* 4 5Ge Zhang* 4Xingwei Qu4Xiang Yue4Chujie Zheng5\n",
      "Jiaheng Liu4Lei Ma3 6Wenhu Chen4Guoyin Wang5Zhaoxiang Zhang2Wenhao Huang4Jiajun Zhang1 2\n",
      "Abstract\n",
      "Instruction tuning enhances large language mod-\n",
      "els (LLMs) to follow human instructions across\n",
      "diverse tasks, relying on high-quality datasets to\n",
      "guide behavior. However, these datasets, whether\n",
      "manually curated or synthetically generated, are\n",
      "[4] (유사도: 0.7652) 051015PCA Component 2Figure 1: Visualization of Projections . The red regions\n",
      "at the bottom represent the pre-training corpus, while the\n",
      "light blue regions above represent the SFT datasets. Darker\n",
      "areas indicate a higher concentration of data points, whereas\n",
      "lighter areas represent sparser distributions. Additional pro-\n",
      "jections are shown in Appendix A.\n",
      "of strong models and are tightly coupled with their gener-\n",
      "ation pipelines, limiting flexibility (Peng et al., 2023; Lian\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    FILE_PATH = \"C:/Users/kowm6/Desktop/test.pdf\"  # 파일 경로 직접 입력\n",
    "    model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "\n",
    "    # PDF 로드\n",
    "    documents = load_pdf(FILE_PATH)\n",
    "\n",
    "    if documents:\n",
    "        # 텍스트 분리 (separator 제거, chunk_size와 chunk_overlap만 사용)\n",
    "        texts = split_text(documents, splitter_type=\"recursive\", chunk_size=500, chunk_overlap=0)\n",
    "        if texts:\n",
    "            hf_embeddings = initialize_embedding(model_name)\n",
    "            if hf_embeddings:\n",
    "                search_query(\n",
    "                    \"\"\"\n",
    "                    1. AITP was designed to address which limitations of existing instruction-tuning datasets?\n",
    "                    \"\"\",\n",
    "                    texts,\n",
    "                    hf_embeddings,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (app)",
   "language": "python",
   "name": "app"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
